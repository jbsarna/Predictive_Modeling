---
title: "Regression Models"
author: "Justin Sarna"
date: "April 14, 2017"
output: html_document
---

```{r}
acs <- read.table("http://jaredlander.com/data/acs_ny.csv",sep=",",
                  header=TRUE, stringsAsFactors=TRUE)
```
```{r}
# Load the necessary libraries
library(ggplot2)
library(useful)
library(caret)
library(ISLR)
library(scales)
library(plyr)
library(rpart)
library(rpart.plot)
library(class)
library(MuMIn)
library(caret)
```
# Create new variables based on EDA
```{r}
# code from hw4 to be used for creating additional models
acs$HighIncome <- as.numeric(with(acs, FamilyIncome >= 250000))
acs$foodstamp_binary <- ifelse(acs$FoodStamp == "Yes",1,0) # (yes = 1, no = 0)
acs$own_home <- ifelse(acs$OwnRent == "Rented",0, ifelse(acs$FamilyIncome == "Mortgage",1,2)) # (own = 1, rent = 0)
acs$family_type_cat <- ifelse(acs$FamilyType == "Married",1, ifelse(acs$FamilyIncome == "Female Head",2,3))
# married = 1, male head = 2, female head = 3
acs$InsuranceHigh <- (acs$Insurance > 1000) * 1
acs$NumWorkers2 <- (acs$NumWorkers == 2) * 1
acs$HouseCostsHigh <- (acs$HouseCosts > 1000) * 1
acs$high_electric <- (acs$ElectricBill > 350) * 1
# Break it into a training and test set with an 80/20 split.
set.seed(447)
testrecs <- sample(nrow(acs),0.2 * nrow(acs))
acs_test <- acs[testrecs,]
acs_fit <- acs[-testrecs,]
# Create binary variable where 1 = not on food stamps & not renting & married
acs$HI_pred1 <- 0
acs$HI_pred1[acs_test$FoodStamp == 'No' & acs_test$OwnRent != 'Rented' & acs_test$FamilyType == 'Married'] <- 1
acs$null_model <- 0
```
Let's start by building a *null* model in which you simply predict that everyone's
income is < 250000 (since the majority of incomes are less than 250000).

# Null Model for model comparison
```{r}
acs$null_model <- 0
```
Create a confusion matrix table and compute the overall accuracy of this model
as well as its sensitivity and specificity.

```{r}
library(caret)
table(acs$HighIncome, acs$null_model)
prop.table(table(acs$HighIncome, acs$null_model))
```
```{r}
confusionMatrix(as.factor(acs$null_model), as.factor(acs$HighIncome), positive = "1")
```

# Logistic Regression

1) specify the model
2) show summary results
3) check fitted values from the model
4) set variable equal to fitted values with probability greater than 50%
5) set variable = adjusted R-squared for model comparison
7) confusion matrix

#### logistic regression model 1
```{r}
logmod1 <- glm(HighIncome ~ FamilyType + NumVehicles + OwnRent + Insurance + YearBuilt, data=acs_fit, 
               family=binomial(link="logit"))
summary(logmod1)
yhat_logmod1 <- (logmod1$fitted.values > 0.05) * 1
log_mod1_aic <- summary(logmod1)$aic
log_cm1 <- confusionMatrix(as.factor(yhat_logmod1), as.factor(acs_fit$HighIncome), positive = "1")
log_cm1
```
#### logistic regression model 2
```{r}
logmod2 <- glm(HighIncome ~ FamilyType + FoodStamp + OwnRent, data=acs_fit, family=binomial(link="logit"))
summary(logmod2)
yhat_logmod2 <- (logmod2$fitted.values > 0.5) * 1
log_mod2_aic <- summary(logmod2)$aic
log_cm2 <- confusionMatrix(as.factor(yhat_logmod2), as.factor(acs_fit$HighIncome), positive = "1")
log_cm2
```
#### logistic regression model 3
```{r}
logmod3 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh + FoodStamp + OwnRent, data=acs_fit, family=binomial(link="logit"))
summary(logmod3)
yhat_logmod3 <- (logmod3$fit > 0.5) * 1
log_mod3_aic <- summary(logmod3)$aic
log_cm3 <- confusionMatrix(as.factor(yhat_logmod3), as.factor(acs_fit$HighIncome), positive = "1")
log_cm3
```
#### logistic regression model 4
```{r}
logmod4 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))
summary(logmod4)
yhat_logmod4 <- (logmod4$fit > 0.5) * 1
log_mod4_aic <- summary(logmod4)$aic
log_cm4 <- confusionMatrix(as.factor(yhat_logmod4), as.factor(acs_fit$HighIncome), positive = "1")
log_cm4
```
#### logistic regression model 5
```{r}
logmod5 <- glm(HighIncome ~ HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))
summary(logmod5)
yhat_logmod5 <- (logmod4$fit > 0.5) * 1
log_mod5_aic <- summary(logmod5)$aic
log_cm5 <- confusionMatrix(as.factor(yhat_logmod5), as.factor(acs_fit$HighIncome), positive = "1")
log_cm5
```
# Linear Regression with predictions

#### linear regression model 1
```{r}
linear_mod1 <- lm(FamilyIncome ~ FamilyType + FoodStamp + OwnRent + HouseCosts + Insurance + ElectricBill + NumRooms, data=acs_fit)
summary(linear_mod1)
#acs_fit$lin_mod1_FamilyIncome <- predict(linear_mod1, newdata=acs_fit)
#acs_fit$lin_mod1_HighIncome <- ifelse(acs_fit$lin_mod1_FamilyIncome > 250000,1,0)
acs_fit$lin_mod1_HighIncome <- ifelse(linear_mod1$fit > 250000,1,0)
linear_mod1_adjR <- summary(linear_mod1)$adj.r.squared
linear_mod1_aic <- AIC(linear_mod1)
linear_cm1 <- confusionMatrix(as.factor(acs_fit$lin_mod1_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
linear_cm1
# Residual Analysis
summary(acs_test$HighIncome - predict(linear_mod1,newdata=acs_test))
```
#### linear regression model 2
```{r}
#ASK ISKEN - I DID THIS ONE DIFFERENT BUT THIS SEEMS TO MAKE MORE SENSE TO ME - PREDICT ON TEST THEN CM
#BUT I THINK THEY BOTH DO THE SAME THING - THE RESULTS ARE THE SAME
linear_mod2 <- lm(FamilyIncome ~ Insurance + HouseCosts + ElectricBill + NumWorkers + FamilyType + FoodStamp + OwnRent + NumBedrooms + NumChildren + NumRooms + NumPeople + NumVehicles + Language, data=acs_fit)
summary(linear_mod2)
acs_fit$lin_mod2_FamilyIncome <- predict(linear_mod2, newdata=acs_fit)
acs_fit$lin_mod2_HighIncome <- ifelse(acs_fit$lin_mod2_FamilyIncome > 250000,1,0)
linear_mod2_adjR <- summary(linear_mod2)$adj.r.squared
linear_mod2_aic <- AIC(linear_mod2)
linear_cm2 <- confusionMatrix(as.factor(acs_fit$lin_mod2_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
linear_cm2
# Residual Analysis
summary(acs_test$HighIncome - predict(linear_mod2,newdata=acs_test))
```
# Support Vector Machines
```{r}
library('kernlab')
```
```{r}
svm_mod1 <- ksvm(HighIncome ~ Insurance + HouseCosts + ElectricBill + FamilyType + OwnRent + NumVehicles + NumBedrooms + 
                   NumWorkers + NumPeople + NumChildren + NumUnits + FoodStamp + YearBuilt + Language + HeatingFuel, 
                 data=acs_test, family=binomial(link="logit"))
acs_fit$svm_HighIncome <- predict(svm_mod1, newdata=acs_fit, type='response')
acs_fit$svm_HighIncome <- (acs_fit$svm_HighIncome > 0.5) * 1
svm_cm1 <- confusionMatrix(as.factor(acs_fit$svm_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
svm_cm1
# Residual Analysis
summary(acs_test$HighIncome - predict(svm_mod1,newdata=acs_test))
```
```{r}
svm_mod2 <- ksvm(FamilyIncome ~ Insurance + HouseCosts + ElectricBill + FamilyType + OwnRent + NumVehicles + NumBedrooms, 
                 data=acs_test)
acs_fit$svm2_HighIncome <- predict(svm_mod2, newdata=acs_fit)
acs_fit$svm2_HighIncome <- ifelse(acs_fit$svm2_HighIncome > 250000,1,0)
svm_cm2 <- confusionMatrix(as.factor(acs_fit$svm2_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
svm_cm2
# Residual Analysis
summary(acs_test$HighIncome - predict(svm_mod2,newdata=acs_test))
```
```{r}
svm_mod3 <- ksvm(HighIncome ~ Insurance + HouseCosts + ElectricBill + FamilyType + OwnRent + NumRooms, data=acs_test,
                 family=binomial(link="logit"))
acs_fit$svm3_HighIncome <- predict(svm_mod3, newdata=acs_fit, type='response')
acs_fit$svm3_HighIncome <- (acs_fit$svm3_HighIncome > 0.5) * 1
svm_cm3 <- confusionMatrix(as.factor(acs_fit$svm3_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
svm_cm3
# Residual Analysis
summary(acs_test$HighIncome - predict(svm_mod3,newdata=acs_test))
```



# Multilevel Model Specification
#### lme()

```{r}
library(nlme)
mms1 <- lme(fixed = HighIncome ~ 1, method = "ML", data = acs_fit, random =~ 1 | Insurance)
summary(mms1)
acs_fit$linest_HighIncome <- ifelse(mms1$fit > 250000,1,0)
mms1_aic <- AIC(mms1)
mms1_aic
```
```{r}
#mms2 <- lme(fixed = FamilyIncome ~ Insurance, method = "ML", data = acs_fit, random =~ FamilyType | HouseCosts)
#summary(mms2)
#mms1_aic <- AIC(mms1)
#mms1_aic
```

# MuMIn and the wonders of the dredge() function!
### MuMIn package must be installed and library called

#### glm dredge
```{r}
# be aware that this takes time to run. It is worth the wait BUT frowned upon when used due to spurious results and inference
glm1 <- glm(HighIncome ~ Insurance + HouseCosts + ElectricBill + FamilyType + FoodStamp + OwnRent + NumBedrooms + NumRooms, family=binomial(logit), na.action = "na.fail", data=acs_fit)
dd_glm1 <- dredge(glm1)
```
```{r}
summary(glm1)
head(dd_glm1)
```
```{r}
# best supported models
subset(dd_glm1, delta < 5)
# best model
subset(dd_glm1, delta == 0)
# calculate variable importance weights
importance(dd_glm1)
```
```{r}
dredge_mod1 <- lm(HighIncome ~ FamilyType + HouseCosts + Insurance + NumRooms + ElectricBill + FoodStamp + OwnRent + NumBedrooms, data = acs_fit, na.action = na.fail)
summary(dredge_mod1)
acs_fit$dredge_HighIncome <- (dredge_mod1$fit > .5) * 1
dredge_mod1_adjR <- summary(dredge_mod1)$adj.r.squared
dredge_mod1_aic <- AIC(dredge_mod1)
#dredge_cm1 <- confusionMatrix(as.factor(acs_fit$linest_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
#dredge_cm1
dredge_mod1_adjR
dredge_mod1_aic
```



### lm dredge
```{r}
lm1 <- lm(FamilyIncome ~ Insurance + HouseCosts + ElectricBill + family_type_cat + FoodStamp + own_home + NumBedrooms + NumRooms, na.action = "na.fail", data=acs_fit)
dd_lm1 <- dredge(lm1)
summary(lm1)
head(dd_lm1)
# best supported models
subset(dd_lm1, delta <5)
# best model
subset(dd_lm1, delta == 0)
# calculate variable importance weights
importance(dd_lm1)
#acs_fit$linest_HighIncome <- ifelse($fit > 250000,1,0)
dredge_mod2_aic <- AIC(dd_lm1)
#dredge_cm1 <- confusionMatrix(as.factor(acs_fit$linest_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
#dredge_cm1
```
# Multilevel Model Specification
#### lme()

```{r}
library(nlme)
mms1 <- lme(fixed = FamilyIncome ~ 1, method = "ML", data = acs_fit, random =~ 1 | HouseCosts)
summary(mms1)
acs_fit$linest_HighIncome <- ifelse(mms1$fit > 250000,1,0)
mms1_aic <- AIC(mms1)
mms1_aic
#mms1_cm <- confusionMatrix(as.factor(acs_fit$linest_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
#mms1_cm
```

```{r}
mms2 <- lme(fixed = HighIncome ~ FamilyType, method = "ML", data = acs_fit, random =~ HouseCosts | HouseCosts)
summary(mms2)
acs_fit$mms2_HighIncome <- ifelse(mms2$fit > 250000,1,0)
#mms2_cm <- confusionMatrix(as.factor(acs_fit$mms2_HighIncome),acs_fit$rand_tree1_pred as.factor(acs_fit$HighIncome), positive = "1")
#mms2_cm
mms2_aic <- AIC(mms2)
mms2_aic
```
```{r}
library(lme4)
lmer1 <- lmer(HighIncome ~ 1 + FoodStamp + OwnRent + (1 | FamilyType), data = acs_fit)
summary(lmer1)
lmer1_aic <- AIC(lmer1)
lmer1_aic
```

# Generalized Additive Models
## Important considerations of GAM
1) They let you represent nonlinear and non-montonic relationships between variables and outcome in linear or logistic regression framework
2) Evaluate the Gam with same measures as you would for simple linear or logistic regression.
```{r}
library(mgcv)
```
```{r}
gam_mod1 <- gam(FamilyIncome ~ s(Insurance) + s(HouseCosts) + s(ElectricBill) + NumWorkers + FamilyType + 
                  FoodStamp + OwnRent + NumBedrooms + s(NumChildren) + s(NumRooms) + s(NumPeople) + NumVehicles + 
                  Language, family=gaussian(link =identity), data=acs_fit)
summary(gam_mod1)
acs_fit$gamest_HighIncome <- ifelse(gam_mod1$fit > 250000,1,0)
gam_mod1_adjR <- summary(gam_mod1)$r.sq
gam_mod1_aic <- AIC(gam_mod1)
gam_cm1 <- confusionMatrix(as.factor(acs_fit$gamest_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
gam_cm1
gam_mod1_aic
# Residual Analysis
summary(acs_test$HighIncome - predict(gam_mod1,newdata=acs_test))
```
```{r}
gam_mod2 <- gam(HighIncome ~ s(Insurance) + s(HouseCosts) + s(ElectricBill) + NumWorkers + FamilyType + FoodStamp + 
                  OwnRent + NumBedrooms + s(NumChildren) + s(NumRooms) + s(NumPeople) + NumVehicles + Language, 
                data = acs_fit, family=binomial(link="logit"))
summary(gam_mod2)
gam_mod2_adjR <- summary(gam_mod2)$r.sq
gam_mod2_aic <- AIC(gam_mod2)
acs_fit$gamest_HighIncome2 <- ifelse(gam_mod2$fit > .5 ,1,0)
gam_cm2 <- confusionMatrix(as.factor(acs_fit$gamest_HighIncome2), as.factor(acs_fit$HighIncome), positive = "1")
gam_cm2
gam_mod2_adjR
gam_mod2_aic
# Residual Analysis
summary(acs_test$HighIncome - predict(gam_mod2,newdata=acs_test))
```

# Regression Model Comparison - List all model's Accuracy, Sensitivity, and 1 additional metric
```{r}
# Display comparison of accuracy of each decision tree 

sprintf("                       The no information rate = %.3f                              ", log_cm1$overall[5])
sprintf("Logistic model 1: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm1$overall['Accuracy'], log_cm1$byClass['Sensitivity'], log_mod1_aic)
sprintf("Logistic model 2: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm2$overall['Accuracy'], log_cm2$byClass['Sensitivity'], log_mod2_aic)
sprintf("Logistic model 3: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm3$overall['Accuracy'], log_cm3$byClass['Sensitivity'], log_mod3_aic)
sprintf("Logistic model 4: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm4$overall['Accuracy'], log_cm4$byClass['Sensitivity'], log_mod4_aic)
sprintf("Logistic model 5: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm5$overall['Accuracy'], log_cm5$byClass['Sensitivity'], log_mod5_aic)
sprintf("GAM model 1:      Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        gam_cm1$overall['Accuracy'], gam_cm1$byClass['Sensitivity'], gam_mod1_aic)
sprintf("GAM model 2:      Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        gam_cm2$overall['Accuracy'], gam_cm2$byClass['Sensitivity'], gam_mod2_aic)

sprintf("MMS model 1:      Predicted Accuracy = NA    Predicted Sensitivity = NA    AIC = %.1f", mms1_aic)

sprintf("Linear model 1:   Predicted Accuracy = %.3f Predicted Sensitivity = %.3f Adj R-squared = %.3f", 
        linear_cm1$overall['Accuracy'], linear_cm1$byClass['Sensitivity'], linear_mod1_adjR)
sprintf("Linear model 2:   Predicted Accuracy = %.3f Predicted Sensitivity = %.3f Adj R-squared = %.3f", 
        linear_cm2$overall['Accuracy'], linear_cm2$byClass['Sensitivity'], linear_mod2_adjR)

```

# Choose best model(s) and display formula

```{r}
formula(gam_mod2)

```

# Comments on logit and linear regression model
1) My linear model does estimate negative incomes, which is not logically or stastically sound
    However, the purpose is to find best predictive model. So I ignored this to see how accurate I could predict High Income
    I also created multi# Multilevel Model Specification
2) Residual analysis of my best LM and best GAM models have a mean very far from 0, with quite a "large" range between min and max. This is not ideal, but for the purposes of stricly predicting as best as possible I can ignore this.