---
title: "Regression Models"
author: "Justin Sarna"
date: "April 14, 2017"
output: html_document
---

```{r}
acs <- read.table("http://jaredlander.com/data/acs_ny.csv",sep=",",
                  header=TRUE, stringsAsFactors=TRUE)
```
```{r}
# Load the necessary libraries
library(ggplot2)
library(useful)
library(caret)
library(ISLR)
library(scales)
library(plyr)
library(rpart)
library(rpart.plot)
library(class)
library(MuMIn)
library(caret)
library(dplyr)
library(RColorBrewer)
library(randomForest)
library(kernlab)
library(nlme)
library(lme4)
library(mgcv)
```
# Create new variables based on EDA
```{r}
# code from hw4 to be used for creating additional models
acs$HighIncome <- as.numeric(with(acs, FamilyIncome >= 250000))
acs$foodstamp_binary <- ifelse(acs$FoodStamp == "Yes",1,0) # (yes = 1, no = 0)
acs$own_home <- ifelse(acs$OwnRent == "Rented",0, ifelse(acs$FamilyIncome == "Mortgage",1,2)) # (own = 1, rent = 0)
acs$family_type_cat <- ifelse(acs$FamilyType == "Married",1, ifelse(acs$FamilyIncome == "Female Head",2,3))

# married = 1, male head = 2, female head = 3
acs$InsuranceHigh <- (acs$Insurance > 1000) * 1
acs$NumWorkers2 <- (acs$NumWorkers == 2) * 1
acs$HouseCostsHigh <- (acs$HouseCosts > 1000) * 1
acs$high_electric <- (acs$ElectricBill > 350) * 1

# Break it into a training and test set with an 80/20 split.
set.seed(447)
testrecs <- sample(nrow(acs),0.2 * nrow(acs))
acs_test <- acs[testrecs,]
acs_fit <- acs[-testrecs,]

# Create binary variable where 1 = not on food stamps & not renting & married
acs$HI_pred1 <- 0
acs$HI_pred1[acs_test$FoodStamp == 'No' & acs_test$OwnRent != 'Rented' & acs_test$FamilyType == 'Married'] <- 1
acs$null_model <- 0
```

Let's start by building a *null* model in which you simply predict that everyone's
income is < 250000 (since the majority of incomes are less than 250000).

## Null Model for model comparison

```{r}
acs$null_model <- 0
```

#### Create a confusion matrix table and compute the overall accuracy of this model as well as its sensitivity and specificity.

```{r}
table(acs$HighIncome, acs$null_model)

prop.table(table(acs$HighIncome, acs$null_model))

confusionMatrix(as.factor(acs$null_model), as.factor(acs$HighIncome), positive = "1")
```

# Logistic Regression

1) specify the model
2) show summary results
3) check fitted values from the model
4) set variable equal to fitted values with probability greater than 50%
5) set variable = adjusted R-squared for model comparison
7) confusion matrix

#### logistic regression model 1

```{r}
logmod1 <- glm(HighIncome ~ FamilyType + NumVehicles + OwnRent + Insurance + YearBuilt, data=acs_fit, 
               family=binomial(link="logit"))
summary(logmod1)
yhat_logmod1 <- (logmod1$fitted.values > 0.05) * 1
log_mod1_aic <- summary(logmod1)$aic
log_cm1 <- confusionMatrix(as.factor(yhat_logmod1), as.factor(acs_fit$HighIncome), positive = "1")
log_cm1
```
#### logistic regression model 2

```{r}
logmod2 <- glm(HighIncome ~ FamilyType + FoodStamp + OwnRent, data=acs_fit, family=binomial(link="logit"))

summary(logmod2)

yhat_logmod2 <- (logmod2$fitted.values > 0.5) * 1

log_mod2_aic <- summary(logmod2)$aic

log_cm2 <- confusionMatrix(as.factor(yhat_logmod2), as.factor(acs_fit$HighIncome), positive = "1")

log_cm2
```
#### logistic regression model 3

```{r}
logmod3 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh + FoodStamp + OwnRent, 
               data=acs_fit, family=binomial(link="logit"))

summary(logmod3)

yhat_logmod3 <- (logmod3$fit > 0.5) * 1

log_mod3_aic <- summary(logmod3)$aic

log_cm3 <- confusionMatrix(as.factor(yhat_logmod3), as.factor(acs_fit$HighIncome), positive = "1")

log_cm3
```
#### logistic regression model 4

```{r}
logmod4 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))

summary(logmod4)

yhat_logmod4 <- (logmod4$fit > 0.5) * 1

log_mod4_aic <- summary(logmod4)$aic

log_cm4 <- confusionMatrix(as.factor(yhat_logmod4), as.factor(acs_fit$HighIncome), positive = "1")

log_cm4
```
#### logistic regression model 5

```{r}
logmod5 <- glm(HighIncome ~ HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))

summary(logmod5)

yhat_logmod5 <- (logmod4$fit > 0.5) * 1

log_mod5_aic <- summary(logmod5)$aic

log_cm5 <- confusionMatrix(as.factor(yhat_logmod5), as.factor(acs_fit$HighIncome), positive = "1")

log_cm5
```
# Linear Regression

#### Linear regression model 1

```{r}
linear_mod1 <- lm(FamilyIncome ~ FamilyType + FoodStamp + OwnRent + HouseCosts + Insurance + ElectricBill + 
                    NumRooms, data=acs_fit)

summary(linear_mod1)

acs_test$lin_mod1_FamilyIncome <- predict(linear_mod1, newdata=acs_test)

acs_test$lin_mod1_HighIncome <- ifelse(acs_test$lin_mod1_FamilyIncome > 250000,1,0)

linear_mod1_adjR <- summary(linear_mod1)$adj.r.squared

linear_mod1_aic <- AIC(linear_mod1)

linear_cm1 <- confusionMatrix(as.factor(acs_test$lin_mod1_HighIncome), as.factor(acs_test$HighIncome), positive = "1")

linear_cm1

# Residual Analysis
summary(acs_test$HighIncome - predict(linear_mod1,newdata=acs_test))
```

#### Linear regression model 2

```{r}
linear_mod2 <- lm(FamilyIncome ~ Insurance + HouseCosts + ElectricBill + NumWorkers + FamilyType + 
                    FoodStamp + OwnRent + NumBedrooms + NumChildren + NumRooms + NumPeople + 
                    NumVehicles + Language, data=acs_fit)

summary(linear_mod2)

acs_test$lin_mod2_FamilyIncome <- predict(linear_mod2, newdata=acs_test)

acs_test$lin_mod2_HighIncome <- ifelse(acs_test$lin_mod2_FamilyIncome > 250000,1,0)

linear_mod2_adjR <- summary(linear_mod2)$adj.r.squared

linear_mod2_aic <- AIC(linear_mod2)

linear_cm2 <- confusionMatrix(as.factor(acs_test$lin_mod2_HighIncome), as.factor(acs_test$HighIncome), positive = "1")

linear_cm2

# Residual Analysis
summary(acs_test$HighIncome - predict(linear_mod2,newdata=acs_test))
```

# Support Vector Machines

"Finds the line/plane/hyperplane that separates the two groups of data as much as possible, and then see which side the new data points land on." click the link below to see rest of SVM explanation. They give two great examples in simple terms in this forum:

https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/

## SVM process

1) Specify the model
2) Predict using model
4) Set variable equal to fitted values with probability greater than 50%
7) Confusion matrix
5) Display confusion matrix

#### SVM model 1

```{r}
svm_mod1 <- ksvm(HighIncome ~ Insurance + HouseCosts + ElectricBill + FamilyType + OwnRent + NumVehicles + NumBedrooms + 
                NumWorkers + NumPeople + NumChildren + NumUnits + FoodStamp + YearBuilt + Language + HeatingFuel, 
                data=acs_fit, family=binomial(link="logit"))

acs_test$svm_HighIncome <- predict(svm_mod1, newdata=acs_test, type='response')

acs_test$svm_HighIncome <- (acs_test$svm_HighIncome > 0.5) * 1

svm_cm1 <- confusionMatrix(as.factor(acs_test$svm_HighIncome), as.factor(acs_test$HighIncome), positive = "1")

svm_cm1

# Residual Analysis
summary(acs_test$HighIncome - predict(svm_mod1,newdata=acs_test))
```

#### SVM model 2

```{r}
svm_mod2 <- ksvm(FamilyIncome ~ Insurance + HouseCosts + ElectricBill + FamilyType + OwnRent + NumVehicles + NumBedrooms, 
                 data=acs_fit)

acs_test$svm2_HighIncome <- predict(svm_mod2, newdata=acs_test)

acs_test$svm2_HighIncome <- ifelse(acs_test$svm2_HighIncome > 250000,1,0)

svm_cm2 <- confusionMatrix(as.factor(acs_test$svm2_HighIncome), as.factor(acs_test$HighIncome), positive = "1")

svm_cm2

# Residual Analysis
summary(acs_test$HighIncome - predict(svm_mod2,newdata=acs_test))
```

#### SVM model 3

```{r}
svm_mod3 <- ksvm(HighIncome ~ Insurance + HouseCosts + ElectricBill + FamilyType + OwnRent + NumRooms, data=acs_fit,
                 family=binomial(link="logit"))

acs_test$svm3_HighIncome <- predict(svm_mod3, newdata=acs_test, type='response')

acs_test$svm3_HighIncome <- (acs_test$svm3_HighIncome > 0.5) * 1

svm_cm3 <- confusionMatrix(as.factor(acs_test$svm3_HighIncome), as.factor(acs_test$HighIncome), positive = "1")

svm_cm3

# Residual Analysis
summary(acs_test$HighIncome - predict(svm_mod3,newdata=acs_test))
```

# Multilevel Model Specification

"A mixed model is similar in many ways to a linear model. It estimates the effects of one or more explanatory variables on a response variable. The output of a mixed model will give you a list of explanatory values, estimates and confidence intervals of their effect sizes, p-values for each effect, and at least one measure of how well the model fits. You should use a mixed model instead of a simple linear model when you have a variable that describes your data sample as a subset of the data you could have collected" Tufts.edu

Check out this link for a very well explained tutorial with easy to understand explanation of mixed models:

http://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html

I would not use this model for this data set to specify the model, but is worth seeing an example of its use and output

1) Specify model
2) Model summary
3) Predict
4) Transform predictions to binomial > .5
4) Confusion matrix
5) Display confusion matrix
6) Set variable = AIC
7) Display AIC


#### MMS model using lme()

```{r}
mms1 <- lme(HighIncome ~ Insurance + HouseCosts + ElectricBill + FoodStamp + 
            OwnRent, method = "ML", data = acs_fit, random =~ NumChildren | FamilyType)

summary(mms1)

acs_test$mms1_HighIncome <- predict(mms1, newdata=acs_test, type='response')
acs_test$mms1_HighIncome <- (acs_test$mms1_HighIncome > 0.5) * 1

mms1_cm <- confusionMatrix(as.factor(acs_test$mms2_HighIncome), as.factor(acs_test$HighIncome), positive = "1")
mms1_cm

mms1_aic <- AIC(mms1)
mms1_aic
```

#### MMS model using lmer()

```{r}
lmer1 <- lmer(HighIncome ~ 1 + FoodStamp + OwnRent + FamilyType + OwnRent + NumWorkers + (1 | FamilyType), data = acs_fit)
summary(lmer1)

acs_test$lmer1_HighIncome <- predict(lmer1, newdata=acs_test, type='response')
acs_test$lmer1_HighIncome <- (acs_test$lmer1_HighIncome > 0.5) * 1

lmer1_cm <- confusionMatrix(as.factor(acs_test$lmer1_HighIncome), as.factor(acs_test$HighIncome), positive = "1")
lmer1_cm

lmer1_aic <- AIC(lmer1)
lmer1_aic
```

# MuMIn and the wonders of the dredge() function!
## This package automates the model specification process to some degree. It does so by "dredging" through all possible combinations of independent variables. You can then display "best models" and importance metrics, which can then be used to specify your model.

This link is a very detailed example of using MuMIn and the dredge function. Scroll down towards the end to see where/how to use dredge:

https://sites.google.com/site/rforfishandwildlifegrads/home/mumin_usage_examples

1) Specify Model
2) Dredge
3) Model Summary
4) Find best models
5) Calculate importance weights

## GLM dredge

```{r}
# be aware that this takes time to run. It is worth the wait BUT frowned upon when used due to spurious results and inference
dredge_glm_mod <- glm(HighIncome ~ Insurance + HouseCosts + ElectricBill + FamilyType + FoodStamp + OwnRent + 
              NumBedrooms + NumRooms, family=binomial(logit), na.action = "na.fail", data=acs_fit)

dd_glm <- dredge(dredge_glm_mod)
```
```{r}
summary(dredge_glm_mod)
```

```{r}
# best supported models
subset(dd_glm, delta < 5)

# best model
subset(dd_glm, delta == 0)

# calculate variable importance weights
importance(dd_glm)
```

#### LM dredge

```{r}
dredge_lm_mod <- lm(HighIncome ~ FamilyType + HouseCosts + Insurance + NumRooms + ElectricBill + FoodStamp + 
                      OwnRent + NumBedrooms, data = acs_fit, na.action = na.fail)

dd_lm <- dredge(dredge_lm_mod)

summary(dredge_lm_mod)

subset(dd_lm, delta < 5)

subset(dd_lm, delta == 0)

importance(dd_lm)
```

# Generalized Additive Models

## Important considerations of GAM

1) They let you represent nonlinear and non-montonic relationships between variables and outcome in linear or logistic regression framework

2) Evaluate the Gam with same measures as you would for simple linear or logistic regression.

Here is a good link with detailed explnation & a good code example:

https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/gam.html

### GAM model process

1) Specify model
2) Model summary
3) Predict using model
4) Use estimates to create binomial for High Income
5) Display r squared
6) Display AIC
7) Confusion matrix
8) Display confusion matrix
9) Residual analysis

#### GAM 1 - Estimating family income & use those estimates to predict High Income

```{r}
gam_mod1 <- gam(FamilyIncome ~ s(Insurance) + s(HouseCosts) + s(ElectricBill) + NumWorkers + FamilyType + 
                  FoodStamp + OwnRent + NumBedrooms + s(NumChildren) + s(NumRooms) + s(NumPeople) + NumVehicles + 
                  Language, family=gaussian(link =identity), data=acs_fit)

summary(gam_mod1)

acs_test$gam_FamilyIncome <- predict(gam_mod1, newdata=acs_test)

acs_test$gam_HighIncome <- ifelse(acs_test$gam_FamilyIncome > 250000,1,0)

gam_mod1_rsq <- summary(gam_mod1)$r.sq

gam_mod_aic <- AIC(gam_mod1)

gam_cm1 <- confusionMatrix(as.factor(acs_test$gam_HighIncome), as.factor(acs_test$HighIncome), positive = "1")

gam_cm1

# Residual Analysis
summary(acs_test$HighIncome - predict(gam_mod1,newdata=acs_test))
```

#### GAM 2 - Use logistic model with GAM to predict High Income

```{r}
gam_mod2 <- gam(HighIncome ~ s(Insurance) + s(HouseCosts) + s(ElectricBill) + NumWorkers + FamilyType + FoodStamp + 
                  OwnRent + NumBedrooms + s(NumChildren) + s(NumRooms) + s(NumPeople) + NumVehicles + Language, 
                data = acs_fit, family=binomial(link="logit"))

summary(gam_mod2)

acs_test$gam2_HighIncome <- predict(gam_mod2, newdata=acs_test)

acs_test$gam2_HighIncome <- ifelse(acs_test$gam2_HighIncome > .5 ,1,0)

gam_mod2_rsq <- summary(gam_mod2)$r.sq

gam_mod2_aic <- AIC(gam_mod2)

gam_cm2 <- confusionMatrix(as.factor(acs_test$gam2_HighIncome), as.factor(acs_test$HighIncome), positive = "1")

gam_cm2

# Residual Analysis
summary(acs_test$HighIncome - predict(gam_mod2,newdata=acs_test))
```

#### Plot GAM model 2 to visualize the s() effect

```{r}
plot(gam_mod2)
```


# Regression Model Comparison - List all model's Accuracy, Sensitivity, and 1 additional metric
```{r}
# Display comparison of accuracy of each decision tree 

sprintf("                       The no information rate = %.3f                              ", log_cm1$overall[5])

sprintf("Logistic model 1: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm1$overall['Accuracy'], log_cm1$byClass['Sensitivity'], log_mod1_aic)

sprintf("Logistic model 2: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm2$overall['Accuracy'], log_cm2$byClass['Sensitivity'], log_mod2_aic)

sprintf("Logistic model 3: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm3$overall['Accuracy'], log_cm3$byClass['Sensitivity'], log_mod3_aic)

sprintf("Logistic model 4: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm4$overall['Accuracy'], log_cm4$byClass['Sensitivity'], log_mod4_aic)

sprintf("Logistic model 5: Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        log_cm5$overall['Accuracy'], log_cm5$byClass['Sensitivity'], log_mod5_aic)

sprintf("GAM model 1:      Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        gam_cm1$overall['Accuracy'], gam_cm1$byClass['Sensitivity'], gam_mod1_aic)

sprintf("GAM model 2:      Predicted Accuracy = %.3f Predicted Sensitivity = %.3f AIC = %.1f", 
        gam_cm2$overall['Accuracy'], gam_cm2$byClass['Sensitivity'], gam_mod2_aic)

sprintf("MMS model 1:      Predicted Accuracy = NA    Predicted Sensitivity = NA    AIC = %.1f", mms1_aic)

sprintf("Linear model 1:   Predicted Accuracy = %.3f Predicted Sensitivity = %.3f Adj R-squared = %.3f", 
        linear_cm1$overall['Accuracy'], linear_cm1$byClass['Sensitivity'], linear_mod1_adjR)
sprintf("Linear model 2:   Predicted Accuracy = %.3f Predicted Sensitivity = %.3f Adj R-squared = %.3f", 
        linear_cm2$overall['Accuracy'], linear_cm2$byClass['Sensitivity'], linear_mod2_adjR)

```

# Choose best model(s) and display formula

```{r}
formula(gam_mod2)
```

# Comments on logit and linear regression model
1) My linear model does estimate negative incomes, which is not logically or stastically sound
    However, the purpose is to find best predictive model. So I ignored this to see how accurate I could predict High Income
    I also created multi# Multilevel Model Specification
2) Residual analysis of my best LM and best GAM models have a mean very far from 0, with quite a "large" range between min and max. This is not ideal, but for the purposes of stricly predicting as best as possible I can ignore this.