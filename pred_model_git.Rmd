---
title: "HW4 - High income household prediction"
output: html_document
Author: Justin Sarna
Class: MIS 680
GitHub: https://github.com/jbsarna
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
acs <- read.table("http://jaredlander.com/data/acs_ny.csv",sep=",",
                  header=TRUE, stringsAsFactors=TRUE)
```
# Load the necessary libraries
```{r}
library(ggplot2)
library(useful)
library(caret)
library(ISLR)
library(scales)
library(plyr)
library(rpart)
library(rpart.plot)
library(class)
library(MuMIn)
```

Let's assume our goal is to build a model to predict if household income is greater 
than $250,000 per year.


# Task 1: Data preparation

We start by building a binary response variable.

```{r}
acs$HighIncome <- as.numeric(with(acs, FamilyIncome >= 250000))
head(acs)
tail(acs)
```

## Before splitting data, I am going to add/modify some variables for use in my models

1) Food Stamp to binary integer for linear regression
2) Own home to binary integer for linear regression
3) Family type to numerical for linear regression later

```{r}
acs$foodstamp_binary <- ifelse(acs$FoodStamp == "Yes",1,0) # (yes = 1, no = 0)
  
  # Option 2 for doing this - I did not use this. Does not add new column but modifes exisiting column
  # levels(acs$FoodStamp) <- c("0","1")

acs$own_home <- ifelse(acs$OwnRent == "Rented",0, ifelse(acs$FamilyIncome == "Mortgage",1,2)) # (own = 1, rent = 0)

acs$family_type_cat <- ifelse(acs$FamilyType == "Married",1, ifelse(acs$FamilyIncome == "Female Head",2,3))
# married = 1, male head = 2, female head = 3
```

### Based on groupby and plots (completed later) create new variables for potential use

```{r}
acs$InsuranceHigh <- (acs$Insurance > 1000) * 1
acs$NumWorkers2 <- (acs$NumWorkers == 2) * 1
acs$HouseCostsHigh <- (acs$HouseCosts > 1000) * 1
acs$high_electric <- (acs$ElectricBill > 350) * 1
```

### Break it into a training and test set with an 80/20 split.

```{r}
set.seed(447)
testrecs <- sample(nrow(acs),0.2 * nrow(acs))
acs_test <- acs[testrecs,]
acs_fit <- acs[-testrecs,]  
```

Create binary variable where 1 = not on food stamps & not renting & married

```{r}
acs$HI_pred1 <- 0
acs$HI_pred1[acs_test$FoodStamp == 'No' & acs_test$OwnRent != 'Rented' & acs_test$FamilyType == 'Married'] <- 1
```

```{r}
# I like this visualization for a quick visual of what I'm dealing with before digging in (modified from class notes)
ggplot(acs,aes(x=FamilyIncome)) + geom_density(fill="#31a354", color="#31a354") +
  geom_vline(xintercept=250000) + scale_x_continuous(label=multiple.dollar, limits=c(0,1000000))
```

### Interesting test I found for testing data normality - downside is it has a 5000 observation limit
If p-value is less than .05 (or chosen significance level) then sample is NOT normally distributed
This is not relevant here, but worth keeping for future
The plot shows that there is a clear left skewned distribution - expected but still nice to include

```{r}
shapiro.test(acs_test$FamilyIncome)
```

# Task 2: Preliminary EDA and feature engineering

Before trying to build any classification models, you should do some exploratory data analysis to
try to get a sense of which variables might be useful for trying to predict cases where FamilyIncome >= 250000. You should use a combination
of group by analysis (i.e. plyr or dplyr or similar) and plotting.  

If you decide you'd like to create some new variables (feature engineering), feel free to do that. Just document what you did and why you did it. 

```{r}
# Get some summary stats on each variable
summary(acs_fit)
```

# Histogram

```{r}
# see that those that those that own home correlate with higher incomes overall
ggplot(acs_fit) + geom_histogram(aes(x=own_home), fill = "gray")
```

# Scatterplots

```{r}
# scatter number of workers and family income
ggplot(data=acs_fit) + geom_point(aes(x=NumWorkers, y=FamilyIncome))

# scatter plot shows that those not on foodstamps tend to have higher income = duh, but relevant for model later
ggplot(data=acs_fit) + geom_point(aes(x=foodstamp_binary, y=FamilyIncome))

# plot shows that homes with 2 workers correlate with higher incomes vs other number of workers
ggplot(data=acs_fit) + geom_point(aes(x=NumWorkers, y=FamilyIncome))

# notice that there are very few observations with male head type. Female head has lower income and married highest incomes
ggplot(data=acs_fit) + geom_point(aes(x=family_type_cat, y=FamilyIncome))

# scatter house costs and family income - see that higher house costs correlate to higher incomes (slightly) - nothin major though
ggplot(data=acs_fit) + geom_point(aes(x=HouseCosts, y=FamilyIncome))
```

```{r}
# create matrix of scatterplots
# pairs(acs[,1:19])
```
# Boxplot

#### coor_cartesian -> Setting limits on the coordinate system will zoom the plot (like you're looking at it with a magnifying glass), and will not change the underlying data like setting limits on a scale will.
```{r}
# See that outliers begin roughly around income of $100,000
ggplot(data=acs_fit) + geom_boxplot(aes(x=NumWorkers, y=FamilyIncome))  + coord_cartesian(ylim = c(0, 350000))
```

# Density Plots

#### These show the density by variable on axis. These are useful to see the concentration range of values
```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$FamilyIncome)) + scale_x_continuous(labels=dollar)
```

```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$HouseCosts)) + scale_x_continuous(labels=dollar)
```
```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$NumChildren)) + scale_x_continuous()
```
```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$FamilyIncome)) + scale_x_log10(breaks =c(100,1000,10000,100000), labels=dollar) + annotation_logticks(sides="bt")
```
```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$HouseCosts)) + scale_x_log10(breaks =c(100,1000,10000,100000), labels=dollar) + annotation_logticks(sides="bt")
```
# Misc Plots
```{r}
# shows positive correlation between insurance and family income
ggplot(acs_fit, aes(x=acs_fit$Insurance, y=acs_fit$FamilyIncome)) +geom_point() + geom_smooth()
```
```{r}
# density plot for electrical bil
ggplot(acs_fit) + geom_density(aes(x=acs_fit$ElectricBill)) + scale_x_log10(breaks =c(100,1000,10000,100000), labels=dollar) + annotation_logticks(sides="bt")

# shows positive correlation between electric bill and family income
ggplot(acs_fit, aes(x=acs_fit$ElectricBill, y=acs_fit$FamilyIncome)) +geom_point() + geom_smooth()
```

# Group by analysis

```{r}
# This shows a good spread or range of each family type group. This will lend itself well to being included in my analysis
ddply(acs_fit,.(FamilyType),summarise,family_type_count=length(FamilyIncome))
```
```{r}
# Interesting look at mean income of family type grouped with home ownership type
ddply(acs_fit,.(FamilyType,OwnRent), summarise, mean_income=mean(FamilyIncome))
```
```{r}
ddply(acs_fit,.(FamilyType,FoodStamp), summarise, mean_income=mean(FamilyIncome))
```
```{r}
# simple look at mean income by foodstamp. Obvious results, but at the same time surprising to find that mean income 
# for those on food stamps in near $50k
ddply(acs_fit,.(FoodStamp), summarise, mean_income=mean(FamilyIncome))
```
```{r}
ddply(acs_fit,.(FoodStamp,NumBedrooms), summarise, mean_income=mean(FamilyIncome), num_bedrooms=mean(NumBedrooms))
```
```{r}
# This is a little excessive, but would be useful for piping it to a csv file (for example) if relevant to tasks at hand
ddply(acs,.(NumBedrooms,NumChildren,NumPeople,NumRooms,NumUnits,NumVehicles,NumWorkers), summarise, mean_income=mean(FamilyIncome))
```
```{r}
ddply(acs_fit,.(OwnRent), summarise, mean_income=mean(FamilyIncome))
```

# Count (family income) by various important indicators/variables

```{r}
# Family Type
tapply(acs_fit$FamilyIncome,acs_fit$FamilyType,length)
tapply(acs_fit$FamilyIncome,acs_fit$FamilyType,mean)
```

```{r}
# Own/Rent
tapply(acs_fit$FamilyIncome,acs_fit$OwnRent,length)
tapply(acs_fit$FamilyIncome,acs_fit$OwnRent,mean)
```

```{r}
# Insurance
tapply(acs_fit$FamilyIncome,acs_fit$FoodStamp,length)
tapply(acs_fit$FamilyIncome,acs_fit$FoodStamp,mean)
```

# Task 3 - Building predictive classifier models using the entire training dataset

Let's start by building a *null* model in which you simply predict that everyone's
income is < 250000 (since the majority of incomes are less than 250000).

```{r}
acs$null_model <- 0
```


Create a confusion matrix table and compute the overall accuracy of this model
as well as its sensitivity and specificity.

```{r}
library(caret)
table(acs$HighIncome, acs$null_model)
prop.table(table(acs$HighIncome, acs$null_model))
```
```{r}
confusionMatrix(as.factor(acs$null_model), as.factor(acs$HighIncome), positive = "1")
```

We would like to build a more accurate model than this.
Your job is to build classifiers to predict the binary HighIncome we created. 
You will be using three different classification
techniques:
* decision trees (use `rpart` package - see Kaggle Titanic example from StatModels2 or session on trees)
* logistic regression (see logistic regression examples we did in StatModels2)
* k-nearest neighbor or some other technique (see kNN example we did in StatModels2)
For each technique, you should:
* build a few models with the training data
* create confusion matrices (using `caret` package) to pick the best fit model for each technique
* use your three best fit models (one from each technique) to predict using the test dataset and evaluate which of the models performs the best
* write a few paragraphs discussing what you did and what you found. In particular, how difficult is it to predict HighIncome? Did one of the techniques outperform the other two?

# Logistic Regression

1) specify the model
2) show summary results
3) check fitted values from the model
4) set variable equal to fitted values with probability greater than 50%
5) show fitted values for visual check of model
6) confusion matrix

#### logistic regression model 1
```{r}
logmod1 <- glm(HighIncome ~ FamilyType + NumVehicles + OwnRent + Insurance + YearBuilt, data=acs_fit, family=binomial(link="logit"))
summary(logmod1)
#logmod1$fitted.values[1:10] - use this to check probabilities
yhat_logmod1 <- (logmod1$fitted.values > 0.05) * 1
#logmod1$y[1:200] - use this to check fitted values
summary(as.factor(yhat_logmod1))
summary(logmod1$fitted.values)
log_cm1 <- confusionMatrix(as.factor(yhat_logmod1), as.factor(acs_fit$HighIncome), positive = "1")
log_cm1
```
#### logistic regression model 2
```{r}
logmod2 <- glm(HighIncome ~ FamilyType + FoodStamp + OwnRent, data=acs_fit, family=binomial(link="logit"))
summary(logmod2)
#logmod2$fitted.values[1:10] - use this to check probabilities
yhat_logmod2 <- (logmod2$fitted.values > 0.5) * 1
#logmod2$y[1:50] - use this to check fitted values
log_cm2 <- confusionMatrix(as.factor(yhat_logmod2), as.factor(acs_fit$HighIncome), positive = "1")
log_cm2
```
#### logistic regression model 3
```{r}
logmod3 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh + FoodStamp + OwnRent, data=acs_fit, family=binomial(link="logit"))
summary(logmod3)
#logmod3$fit[18100:18196] - use this to check probabilities
yhat_logmod3 <- (logmod3$fit > 0.5) * 1
#logmod3$y[18000:18196] - use this to check fitted values
log_cm3 <- confusionMatrix(as.factor(yhat_logmod3), as.factor(acs_fit$HighIncome), positive = "1")
log_cm3
```
#### logistic regression model 4
```{r}
logmod4 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))
summary(logmod4)
#logmod4$fit[18100:18196] - use this to check probabilities
yhat_logmod4 <- (logmod4$fit > 0.5) * 1
#logmod4$y[18000:18196] - use this to check fitted values
log_cm4 <- confusionMatrix(as.factor(yhat_logmod4), as.factor(acs_fit$HighIncome), positive = "1")
```
#### logistic regression model 5
```{r}
logmod5 <- glm(HighIncome ~ HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))
summary(logmod5)
#logmod5$fit[18100:18196] - use this to check probabilities
yhat_logmod5 <- (logmod4$fit > 0.5) * 1
#logmod5$y[18000:18196] - use this to check fitted values
log_cm5 <- confusionMatrix(as.factor(yhat_logmod5), as.factor(acs_fit$HighIncome), positive = "1")
log_cm5
```
# Linear Regression with predictions

#### linear regression model 1
```{r}
# First linear regression to estimate family income with test data set
linear_mod1 <- lm(FamilyIncome ~ FamilyType + FoodStamp + OwnRent + HouseCosts + Insurance + ElectricBill + NumRooms, data=acs_fit)
summary(linear_mod1)
```
#### linear regression model 2
```{r}
# Created another linear regression to estimate family income with test data set to be used for predicting high income
linear_mod2 <- lm(FamilyIncome ~ Insurance + HouseCosts + ElectricBill + NumWorkers + FamilyType + FoodStamp + OwnRent + NumBedrooms + NumChildren + NumRooms + NumPeople + NumVehicles + Language, data=acs_fit)
summary(linear_mod2)
linear_mod2$fitted.values[1:25]
acs_fit$linest_HighIncome <- ifelse(linear_mod2$fit > 250000,1,0)
linear_cm2 <- confusionMatrix(as.factor(acs_fit$linest_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")
linear_cm2
```
## Regression Model Comparison
Could not get it to work how I wanted - I have the big chunk of code if you want to see it!

# Comments on logit and linear regression model
1) My linear model does estimate negative incomes, which is not logically or stastically sound
    However, the purpose is to find best predictive model. So I ignored this to see how accurate I could predict High Income
    I also created multiple linear regressions, and chose this one. I based my decision on R-squared and changes in adjusted R-
    squared as I added/subtracted variables.

2) Best regression model = linear regression model 2 - based on accuracy at .941 (null model = .9389) and sensitivity at .23277

# DECISION TREES

#### Decision tree 1
```{r}
tree1 <- rpart(HighIncome ~ FamilyType + HouseCosts + NumWorkers2 + OwnRent + Insurance + NumWorkers2 + YearBuilt + NumBedrooms, data=acs_fit, method="class")
rpart.plot(tree1)
tree1
head(predict(tree1))
head(predict(tree1, type="class"))
tree1_cm <- confusionMatrix(predict(tree1, type="class"), acs_fit$HighIncome, positive = "1")
tree1_cm
```
#### Decision tree 2
```{r}
tree2 <- rpart(HighIncome ~ FoodStamp + Insurance + FamilyType, data=acs_fit, method="class", control=rpart.control(minsplit=2, cp=0))
rpart.plot(tree2)
tree2
head(predict(tree2))
head(predict(tree2, type="class"))
tree2_cm <- confusionMatrix(predict(tree2, type="class"), acs_fit$HighIncome, positive = "1")
tree2_cm
```
#### Decision tree 3
```{r}
tree3 <- rpart(HighIncome ~ Insurance + ElectricBill + HouseCosts, data=acs_fit, method="class", control=rpart.control(minsplit=2, cp=.005))
rpart.plot(tree3)
tree3
head(predict(tree3))
head(predict(tree3, type="class"))
tree3_cm <- confusionMatrix(predict(tree3, type="class"), acs_fit$HighIncome, positive = "1")
tree3_cm
```
#### Decision tree 4
```{r}
tree4 <- rpart(HighIncome ~ Insurance + ElectricBill + HouseCosts + NumBedrooms + NumChildren + NumPeople + NumRooms + NumVehicles + NumWorkers + FoodStamp + OwnRent + ElectricBill + HouseCosts, data=acs_fit, method="class", control=rpart.control(minsplit=2, cp=0))
rpart.plot(tree4)
tree4
head(predict(tree4))
head(predict(tree4, type="class"))
tree4_cm <- confusionMatrix(predict(tree4, type="class"), acs_fit$HighIncome, positive = "1")
tree4_cm
```

## Tree Comparison

```{r}
# make predictions using test data
tree1_pred <- predict(tree1, acs_test, type="class" )
tree2_pred <- predict(tree2, acs_test, type="class" ) 
tree3_pred <- predict(tree3, acs_test, type="class" ) 
tree4_pred <- predict(tree4, acs_test, type="class" )

# Confusion matrices
tree_cm1_pred <- confusionMatrix(tree1_pred, acs_test$HighIncome, positive = "1")
tree_cm2_pred <- confusionMatrix(tree2_pred, acs_test$HighIncome, positive = "1")
tree_cm3_pred <- confusionMatrix(tree3_pred, acs_test$HighIncome, positive = "1")
tree_cm4_pred <- confusionMatrix(tree4_pred, acs_test$HighIncome, positive = "1")

# Display comparison of accuracy of each decision tree 
sprintf("Tree1: Fit acc = %.3f Pred acc = %.3f",tree1_cm$overall['Accuracy'], tree_cm1_pred$overall['Accuracy'])
sprintf("Tree2: Fit acc = %.3f Pred acc = %.3f",tree2_cm$overall['Accuracy'], tree_cm2_pred$overall['Accuracy'])
sprintf("Tree3: Fit acc = %.3f Pred acc = %.3f",tree3_cm$overall['Accuracy'], tree_cm3_pred$overall['Accuracy'])
sprintf("Tree4: Fit acc = %.3f Pred acc = %.3f",tree4_cm$overall['Accuracy'], tree_cm4_pred$overall['Accuracy'])
```
## Decision Tree Decision

1) At first glance tree 4 would appear the best choice. However, this tree has a nearly 100% accuracy in the fit data BUT the lowest 
    prediction accuracy. This can be seen through its extreme complexity which leads to a .99 sensitivity. This is like overfitting 
    that leads to "garbage" predictions unless the samples are nearly identical
2) I would choose Tree 3 for two reasons - First, it has highest prediction accuracy. Second, it has a pretty low sensitivity at .14

# k-nearest neighbor

## First Normalize the dataset
```{r}
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
```
```{r}
acs_numericals <- data.frame(acs$NumBedrooms, acs$NumChildren, acs$NumPeople, acs$NumRooms, acs$NumVehicles, acs$NumWorkers, acs$HouseCosts, acs$ElectricBill, acs$Insurance)
acs_norm <- as.data.frame(lapply(acs_numericals[1:8], normalize))
acs_norm$HighIncome1 <- c(acs$HighIncome)
```
```{r}
# Count rows
m <- nrow(acs_numericals)

# Create index of random row nums for the validation set
val <- sample(1:m, size = round(m/3))

# Create the learning and validation sets
acsNorm_learn <- acs_norm[-val,]
acsNorm_valid <- acs_norm[val,]
```

```{r}
summary(acs_norm)
```
#### knn 1

```{r}
acs_knn1 <- knn(acsNorm_learn[,1:8], acsNorm_valid[,1:8], acsNorm_learn$HighIncome1, k=5, prob = TRUE)
head(acs_knn1)

# Show the classification matrix
table(acsNorm_valid$HighIncome1, acs_knn1)

pcol1 <- as.character(as.numeric(acsNorm_valid$HighIncome1))
pairs(acsNorm_valid[1:8], pch = pcol1, col = c("green3", "red")
  [(acsNorm_valid$HighIncome1 != acs_knn1)+1])

# confusion matrix for knn1
knn1_cm1_pred <- confusionMatrix(acs_knn1, acsNorm_valid$HighIncome, positive = "1")
knn1_cm1_pred
```

```{r}
# confusion matrix for knn1
knn1_cm1_pred <- confusionMatrix(acs_knn1, acsNorm_valid$HighIncome, positive = "1")
knn1_cm1_pred
```

#### knn 2
```{r}
acs_knn2 <- knn(acsNorm_learn[,1:4], acsNorm_valid[,1:4], acsNorm_learn$HighIncome, k=5, prob = TRUE)
head(acs_knn2)
table(acsNorm_valid$HighIncome, acs_knn2)

pcol2 <- as.character(as.numeric(acsNorm_valid$HighIncome1))
pairs(acsNorm_valid[2:5], pch = pcol2, col = c("green3", "red")
  [(acsNorm_valid$HighIncome1 != acs_knn2)+1])

knn2_cm2_pred <- confusionMatrix(acs_knn2, acsNorm_valid$HighIncome, positive = "1")
knn2_cm2_pred
```

#### knn 3

```{r}
acs_knn3 <- knn(acsNorm_learn[,6:8], acsNorm_valid[,6:8], acsNorm_learn$HighIncome, k=3, prob = TRUE)
head(acs_knn3)
table(acsNorm_valid$HighIncome, acs_knn3)

pcol3 <- as.character(as.numeric(acsNorm_valid$HighIncome1))
pairs(acsNorm_valid[6:8], pch = pcol3, col = c("green3", "red")
  [(acsNorm_valid$HighIncome1 != acs_knn3)+1])

knn3_cm3_pred <- confusionMatrix(acs_knn3, acsNorm_valid$HighIncome, positive = "1")
knn3_cm3_pred
```
#### knn 4

```{r}
acs_knn4 <- knn(acsNorm_learn[,1:8], acsNorm_valid[,1:8], acsNorm_learn$HighIncome, k=10, prob = TRUE)
head(acs_knn4)
table(acsNorm_valid$HighIncome, acs_knn4)

pcol4 <- as.character(as.numeric(acsNorm_valid$HighIncome1))
pairs(acsNorm_valid[1:8], pch = pcol4, col = c("green3", "red")
  [(acsNorm_valid$HighIncome1 != acs_knn4)+1])

knn4_cm4_pred <- confusionMatrix(acs_knn4, acsNorm_valid$HighIncome, positive = "1")
knn4_cm4_pred
```
#### knn 5
```{r}
acs_knn5 <- knn(acsNorm_learn[,1:8], acsNorm_valid[,1:8], acsNorm_learn$HighIncome, k=25, prob = TRUE)
head(acs_knn5)
table(acsNorm_valid$HighIncome, acs_knn5)

pcol5 <- as.character(as.numeric(acsNorm_valid$HighIncome1))
pairs(acsNorm_valid[1:8], pch = pcol5, col = c("green3", "red")
  [(acsNorm_valid$HighIncome1 != acs_knn5)+1])

knn5_cm5_pred <- confusionMatrix(acs_knn5, acsNorm_valid$HighIncome, positive = "1")
knn5_cm5_pred
```
#### knn 6
```{r}
acs_knn6 <- knn(acsNorm_learn[,1:8], acsNorm_valid[,1:8], acsNorm_learn$HighIncome, k=50, prob = TRUE)
head(acs_knn6)
table(acsNorm_valid$HighIncome, acs_knn6)

pcol6 <- as.character(as.numeric(acsNorm_valid$HighIncome1))
pairs(acsNorm_valid[1:8], pch = pcol6, col = c("green3", "red")
  [(acsNorm_valid$HighIncome1 != acs_knn6)+1])

knn6_cm6_pred <- confusionMatrix(acs_knn6, acsNorm_valid$HighIncome, positive = "1")
knn6_cm6_pred
```
## K nearest neighbor decision

1) As I increase k the accuract increases until I chose k = 50. I could continue to repeat this property or (loop) to find the best 
    k value where change in accuracy = 0 (first derivative of the function).

2) One thing to be carefule of is that the higher the k value the more complex the "outline" of the determinations.

3) I would also think it be valuable to change the variables included in knn. I only did this with two sets of variables, and mostly
    focused on changing the size of k.
    
# MuMIn and the wonders of the dredge() function!
### MuMIn package must be installed and library called

#### glm dredge
```{r}
# be aware that this takes time to run. It is worth the wait BUT frowned upon when used due to spurious results and inference
glm1 <- glm(HighIncome ~ Insurance + HouseCosts + ElectricBill + FamilyType + FoodStamp + OwnRent + NumBedrooms + NumRooms, family=binomial(logit), na.action = "na.fail", data=acs_fit)
dd_glm1 <- dredge(glm1)
```
```{r}
summary(glm1)
dd_glm1
```
```{r}
# best supported models
subset(dd_glm1, delta < 5)

# best model
subset(dd_glm1, delta == 0)

# calculate variable importance weights
importance(dd_glm1) 
```


### lm dredge
```{r}
lm1 <- lm(FamilyIncome ~ Insurance + HouseCosts + ElectricBill + family_type_cat + FoodStamp + own_home + NumBedrooms + NumRooms, na.action = "na.fail", data=acs_fit)
dd_lm1 <- dredge(lm1)
summary(lm1)
dd_lm1
```
```{r}
# best supported models
subset(dd_lm1, delta <5)

# best model
subset(dd_lm1, delta == 0)

# calculate variable importance weights
importance(dd_lm1) 
```
# Multilevel Model Specification
#### lme()

```{r}
summary(acs_norm2)
```
```{r}
library(nlme)
mms1 <- lme(fixed = FamilyIncome ~ 1, method = "ML", data = acs_fit, random =~ 1 | HouseCosts)
summary(mms1)
```
```{r}
mms2 <- lme(fixed = HighIncome ~ FamilyType, data=acsNorm_learn2, random =~ FamilyType | Insurance)
summary(mms2)
```


